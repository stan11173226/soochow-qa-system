# ä¿®å¾©ç‰ˆæœ¬ - test0811_fixed.py

import logging
from typing import List, Dict, Any, Tuple
from langchain_community.llms import Ollama
# æ›´æ–°çš„åŒ¯å…¥æ–¹å¼
try:
    from langchain_huggingface import HuggingFaceEmbeddings
    print("âœ… ä½¿ç”¨æ–°ç‰ˆ langchain_huggingface")
except ImportError:
    from langchain_community.embeddings import HuggingFaceEmbeddings
    print("âš ï¸ ä½¿ç”¨èˆŠç‰ˆ langchain_communityï¼Œå»ºè­°å‡ç´š")

from langchain_chroma import Chroma
from langchain.docstore.document import Document
import streamlit as st
import asyncio
import httpx
import torch
import json
import os
import shutil
from datetime import datetime
from huggingface_hub import snapshot_download

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('qa_system.log')
    ]
)

def download_and_verify_model():
    """ä¸‹è¼‰ä¸¦é©—è­‰åµŒå…¥æ¨¡å‹"""
    model_name = "GanymedeNil/text2vec-large-chinese"
    local_dir = "./text2vec-large-chinese"
    
    # æª¢æŸ¥æ¨¡å‹ç›®éŒ„æ˜¯å¦å­˜åœ¨ä¸”åŒ…å«å¿…è¦æ–‡ä»¶
    required_files = ['pytorch_model.bin', 'config.json', 'tokenizer.json']
    model_complete = False
    
    if os.path.exists(local_dir):
        existing_files = os.listdir(local_dir)
        model_complete = any(
            file in existing_files for file in ['pytorch_model.bin', 'model.safetensors']
        ) and 'config.json' in existing_files
        
        if model_complete:
            logging.info(f"æ¨¡å‹å·²å­˜åœ¨ä¸”å®Œæ•´: {local_dir}")
            return True
        else:
            logging.warning(f"æ¨¡å‹ç›®éŒ„å­˜åœ¨ä½†ä¸å®Œæ•´ï¼Œå°‡é‡æ–°ä¸‹è¼‰")
            try:
                shutil.rmtree(local_dir)
            except Exception as e:
                logging.error(f"ç„¡æ³•åˆªé™¤ä¸å®Œæ•´çš„æ¨¡å‹ç›®éŒ„: {e}")
                return False
    
    # ä¸‹è¼‰æ¨¡å‹
    try:
        logging.info(f"é–‹å§‹ä¸‹è¼‰æ¨¡å‹: {model_name}")
        print(f"ğŸ“¥ æ­£åœ¨ä¸‹è¼‰æ¨¡å‹ '{model_name}'ï¼Œè«‹ç¨å€™...")
        
        snapshot_download(
            repo_id=model_name,
            local_dir=local_dir,
            resume_download=True,  # æ”¯æ´æ–·é»çºŒå‚³
            local_dir_use_symlinks=False  # ä¸ä½¿ç”¨ç¬¦è™Ÿé€£çµ
        )
        
        # é©—è­‰ä¸‹è¼‰æ˜¯å¦æˆåŠŸ
        if os.path.exists(local_dir):
            downloaded_files = os.listdir(local_dir)
            model_file_exists = any(
                file in downloaded_files for file in ['pytorch_model.bin', 'model.safetensors']
            )
            config_exists = 'config.json' in downloaded_files
            
            if model_file_exists and config_exists:
                logging.info(f"âœ… æ¨¡å‹ä¸‹è¼‰æˆåŠŸ: {local_dir}")
                print(f"âœ… æ¨¡å‹ä¸‹è¼‰å®Œæˆ")
                return True
            else:
                logging.error(f"æ¨¡å‹ä¸‹è¼‰ä¸å®Œæ•´ï¼Œç¼ºå°‘å¿…è¦æ–‡ä»¶")
                return False
        else:
            logging.error(f"ä¸‹è¼‰å¾Œæ¨¡å‹ç›®éŒ„ä¸å­˜åœ¨")
            return False
            
    except Exception as e:
        logging.error(f"ä¸‹è¼‰æ¨¡å‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        print(f"âŒ æ¨¡å‹ä¸‹è¼‰å¤±æ•—: {e}")
        return False

class ConversationMemory:
    """å°è©±è¨˜æ†¶ç®¡ç†é¡"""
    
    def __init__(self, max_history: int = 10):
        self.max_history = max_history
        self.conversation_history = []
        
    def add_exchange(self, user_input: str, system_response: str, context_used: str = ""):
        """æ·»åŠ ä¸€çµ„å°è©±äº¤æ›"""
        exchange = {
            'timestamp': datetime.now().isoformat(),
            'user_input': user_input,
            'system_response': system_response,
            'context_used': context_used
        }
        
        self.conversation_history.append(exchange)
        
        # ä¿æŒæ­·å²è¨˜éŒ„åœ¨æœ€å¤§é™åˆ¶å…§
        if len(self.conversation_history) > self.max_history:
            self.conversation_history = self.conversation_history[-self.max_history:]
    
    def get_conversation_context(self, current_query: str) -> str:
        """ç”Ÿæˆå°è©±ä¸Šä¸‹æ–‡å­—ä¸²"""
        if not self.conversation_history:
            return ""
        
        context_parts = ["=== å°è©±æ­·å² ==="]
        
        for i, exchange in enumerate(self.conversation_history[-5:], 1):  # åªå–æœ€è¿‘5è¼ªå°è©±
            context_parts.append(f"ç¬¬{i}è¼ªå°è©±:")
            context_parts.append(f"ä½¿ç”¨è€…å•: {exchange['user_input']}")
            context_parts.append(f"ç³»çµ±ç­”: {exchange['system_response'][:200]}...")  # æˆªå–å‰200å­—
            context_parts.append("---")
        
        context_parts.append(f"ç•¶å‰å•é¡Œ: {current_query}")
        context_parts.append("=== å°è©±æ­·å²çµæŸ ===\n")
        
        return "\n".join(context_parts)
    
    def clear_history(self):
        """æ¸…é™¤å°è©±æ­·å²"""
        self.conversation_history = []
    
    def get_related_previous_qa(self, current_query: str) -> List[Dict]:
        """æ ¹æ“šç•¶å‰å•é¡Œæ‰¾å‡ºç›¸é—œçš„æ­·å²å°è©±"""
        if not self.conversation_history:
            return []
        
        # ç°¡å–®çš„é—œéµå­—åŒ¹é…é‚è¼¯
        current_keywords = set(current_query.lower().split())
        related_exchanges = []
        
        for exchange in self.conversation_history:
            previous_keywords = set(exchange['user_input'].lower().split())
            # å¦‚æœæœ‰å…±åŒé—œéµå­—ï¼Œèªç‚ºæ˜¯ç›¸é—œçš„
            if current_keywords & previous_keywords:
                related_exchanges.append(exchange)
        
        return related_exchanges[-3:]  # è¿”å›æœ€è¿‘3å€‹ç›¸é—œå°è©±

@st.cache_resource
def get_embeddings() -> HuggingFaceEmbeddings:
    """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ï¼ŒåŒ…å«å®Œæ•´çš„éŒ¯èª¤è™•ç†"""
    logging.info("æ­£åœ¨åˆå§‹åŒ–åµŒå…¥æ¨¡å‹...")
    
    # é¦–å…ˆç¢ºä¿æ¨¡å‹å·²ä¸‹è¼‰
    if not download_and_verify_model():
        st.error("âŒ æ¨¡å‹ä¸‹è¼‰å¤±æ•—ï¼Œè«‹æª¢æŸ¥ç¶²è·¯é€£æ¥æˆ–å˜—è©¦é‡æ–°å•Ÿå‹•")
        st.stop()
    
    # æª¢æ¸¬ç¡¬é«”
    if torch.cuda.is_available():
        device = 'cuda'
        logging.info("æª¢æ¸¬åˆ° NVIDIA GPUï¼Œä½¿ç”¨ CUDA åŠ é€Ÿ")
    elif torch.backends.mps.is_available():
        device = 'mps'
        logging.info("æª¢æ¸¬åˆ° Apple Silicon GPUï¼Œä½¿ç”¨ MPS åŠ é€Ÿ")
    else:
        device = 'cpu'
        logging.info("æœªæª¢æ¸¬åˆ° GPUï¼Œä½¿ç”¨ CPU")
    
    try:
        # å˜—è©¦è¼‰å…¥æ¨¡å‹
        embeddings = HuggingFaceEmbeddings(
            model_name="./text2vec-large-chinese",
            model_kwargs={'device': device}
        )
        logging.info("âœ… åµŒå…¥æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        return embeddings
        
    except Exception as e:
        logging.error(f"åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±æ•—: {e}")
        
        # å˜—è©¦å‚™ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨ç·šä¸Šæ¨¡å‹
        st.warning("âš ï¸ æœ¬åœ°æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼Œå˜—è©¦ä½¿ç”¨ç·šä¸Šå‚™ç”¨æ¨¡å‹...")
        try:
            embeddings = HuggingFaceEmbeddings(
                model_name="GanymedeNil/text2vec-large-chinese",
                model_kwargs={'device': device}
            )
            logging.info("âœ… ä½¿ç”¨ç·šä¸Šæ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
            return embeddings
        except Exception as e2:
            logging.error(f"ç·šä¸Šæ¨¡å‹ä¹Ÿå¤±æ•—: {e2}")
            st.error(f"âŒ æ‰€æœ‰æ¨¡å‹è¼‰å…¥æ–¹å¼éƒ½å¤±æ•—äº†")
            st.error("è«‹å˜—è©¦ä»¥ä¸‹è§£æ±ºæ–¹æ¡ˆï¼š")
            st.error("1. åˆªé™¤ './text2vec-large-chinese' è³‡æ–™å¤¾å¾Œé‡æ–°å•Ÿå‹•")
            st.error("2. æª¢æŸ¥ç¶²è·¯é€£æ¥")
            st.error("3. ç¢ºä¿æœ‰è¶³å¤ çš„ç£ç¢Ÿç©ºé–“")
            st.stop()

@st.cache_resource
def get_llm() -> Ollama:
    logging.info("æ­£åœ¨åˆå§‹åŒ– LLM æ¨¡å‹...")
    try:
        llm = Ollama(model="gemma3n:latest")
        logging.info("âœ… LLM æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        return llm
    except Exception as e:
        logging.error(f"LLM åˆå§‹åŒ–å¤±æ•—: {e}")
        st.error("âŒ LLM æ¨¡å‹åˆå§‹åŒ–å¤±æ•—ï¼Œè«‹ç¢ºä¿ Ollama æœå‹™æ­£åœ¨é‹è¡Œ")
        st.error("è«‹åŸ·è¡Œï¼šollama serve")
        st.stop()

class SoochowQASystem:
    RELEVANCE_SCORE_THRESHOLD = -600.0

    FALLBACK_MAP = {
        ('è¨»å†Š', 'é¸èª²', 'æˆç¸¾', 'å­¸åˆ†', 'ç•¢æ¥­', 'é›™ä¸»ä¿®', 'è¼”ç³»', 'æ‹›ç”Ÿ', 'èª²å‹™'): {
            'name': 'æ±å³å¤§å­¸æ•™å‹™è™•',
            'url': 'http://www.scu.edu.tw/acad/'
        },
        ('å®¿èˆ', 'ä½å®¿', 'åºŠä½', 'ç¤¾åœ˜', 'è«‹å‡', 'æ“è¡Œ', 'å¾·è‚²', 'ç¾¤è‚²', 'å¿ƒç†è«®å•†', 'å¥åº·'): {
            'name': 'æ±å³å¤§å­¸å­¸ç”Ÿäº‹å‹™è™•',
            'url': 'https://www.scu.edu.tw/osa/'
        },
        ('æ¡è³¼', 'ç¹³è²»', 'å‡ºç´', 'å ´åœ°', 'ç‡Ÿç¹•', 'ç’°å®‰', 'æ ¡åœ’å®‰å…¨'): {
            'name': 'æ±å³å¤§å­¸ç¸½å‹™è™•',
            'url': 'https://www.scu.edu.tw/oga/'
        },
        ('ç ”ç©¶è¨ˆç•«', 'å­¸è¡“å€«ç†', 'æ ¡å‹™ç™¼å±•', 'è©•é‘‘'): {
            'name': 'æ±å³å¤§å­¸ç ”ç©¶ç™¼å±•è™•',
            'url': 'https://www.scu.edu.tw/ord/'
        },
        ('åœ–æ›¸é¤¨', 'å€Ÿæ›¸', 'é‚„æ›¸', 'è‡ªç¿’', 'è³‡æ–™åº«'): {
            'name': 'æ±å³å¤§å­¸åœ–æ›¸é¤¨',
            'url': 'https://www.library.scu.edu.tw/'
        },
        ('è³‡æ–™ç§‘å­¸ç³»', 'è³‡ç§‘ç³»', 'å¤§æ•¸æ“š', 'å·¨é‡è³‡æ–™'): {
            'name': 'æ±å³å¤§å­¸å·¨é‡è³‡æ–™ç®¡ç†å­¸é™¢',
            'url': 'https://bigdata.scu.edu.tw/'
        }
    }
    
    DEFAULT_FALLBACK = {
        'name': 'æ±å³å¤§å­¸',
        'url': 'http://www.scu.edu.tw/'
    }

    def __init__(self):
        try:
            self.embeddings = get_embeddings()
            self.llm = get_llm()
            self.vector_store = self._init_vector_store()
            
            # åˆå§‹åŒ–å°è©±è¨˜æ†¶
            if "conversation_memory" not in st.session_state:
                st.session_state.conversation_memory = ConversationMemory()
            self.memory = st.session_state.conversation_memory
            
        except Exception as e:
            logging.error(f"SoochowQASystem åˆå§‹åŒ–å¤±æ•—: {e}")
            raise

    def _init_vector_store(self) -> Chroma:
        if "vector_store" not in st.session_state:
            logging.info("æ­£åœ¨è¼‰å…¥ Chroma å‘é‡è³‡æ–™åº«...")
            try:
                if not os.path.exists("chroma_db"):
                    st.error("âŒ æ‰¾ä¸åˆ° 'chroma_db' è³‡æ–™åº«ç›®éŒ„")
                    st.error("è«‹å…ˆåŸ·è¡Œ test1.py ä¾†å»ºç«‹å‘é‡è³‡æ–™åº«")
                    st.stop()
                
                st.session_state["vector_store"] = Chroma(
                    persist_directory="chroma_db",
                    embedding_function=self.embeddings
                )
                logging.info("âœ… å‘é‡è³‡æ–™åº«è¼‰å…¥æˆåŠŸ")
            except Exception as e:
                logging.error(f"å‘é‡è³‡æ–™åº«è¼‰å…¥å¤±æ•—: {e}")
                st.error("âŒ å‘é‡è³‡æ–™åº«è¼‰å…¥å¤±æ•—")
                st.error("è«‹ç¢ºä¿ 'chroma_db' ç›®éŒ„å­˜åœ¨ä¸”å®Œæ•´")
                st.stop()
                
        return st.session_state["vector_store"]
        
    def _get_fallback_suggestion(self, query: str) -> Dict[str, str]:
        for keywords, suggestion in self.FALLBACK_MAP.items():
            if any(keyword in query for keyword in keywords):
                logging.info(f"è§¸ç™¼å›é€€æ©Ÿåˆ¶ï¼ŒåŒ¹é…é—œéµå­—: {keywords}")
                return suggestion
        logging.info("è§¸ç™¼å›é€€æ©Ÿåˆ¶ï¼Œä½¿ç”¨é è¨­å»ºè­°")
        return self.DEFAULT_FALLBACK
        
    async def retrieve_relevant_documents(self, query: str) -> List[Tuple[Document, float]]:
        try:
            results_with_scores = await self.vector_store.asimilarity_search_with_relevance_scores(
                query=query,
                k=10
            )
            return results_with_scores
        except Exception as e:
            logging.error(f"æ–‡æª”æª¢ç´¢éŒ¯èª¤: {e}", exc_info=True)
            raise

    async def generate_response_with_memory(self, query: str, context: str) -> str:
        """ç”Ÿæˆå¸¶æœ‰å°è©±è¨˜æ†¶çš„å›æ‡‰"""
        
        # ç²å–å°è©±æ­·å²ä¸Šä¸‹æ–‡
        conversation_context = self.memory.get_conversation_context(query)
        
        # ç²å–ç›¸é—œçš„æ­·å²å•ç­”
        related_previous_qa = self.memory.get_related_previous_qa(query)
        
        # æ§‹å»ºå¢å¼·çš„æç¤º
        memory_context = ""
        if related_previous_qa:
            memory_context = "\nã€ç›¸é—œæ­·å²å°è©±ã€‘:\n"
            for i, qa in enumerate(related_previous_qa, 1):
                memory_context += f"æ­·å²å•é¡Œ{i}: {qa['user_input']}\n"
                memory_context += f"æ­·å²å›ç­”{i}: {qa['system_response'][:150]}...\n\n"
        
        prompt = f"""
===== æ±å³å¤§å­¸å°è©±å¼æ™ºèƒ½çŸ¥è­˜å¼•æ“ =====

ä½ æ˜¯æ±å³å¤§å­¸çš„AIåŠ©ç†ï¼Œèƒ½å¤ è¨˜ä½å°è©±æ­·å²ä¸¦æä¾›é€£è²«çš„å›æ‡‰ã€‚

{conversation_context}

{memory_context}

**å¢å¼·å°è©±æŒ‡å°åŸå‰‡ï¼š**
1. **å°è©±é€£è²«æ€§**ï¼šåƒè€ƒå°è©±æ­·å²ï¼Œå»ºç«‹å•é¡Œé–“çš„é€£æ¥
2. **å€‹äººåŒ–å›æ‡‰**ï¼šæ ¹æ“šç”¨æˆ¶æå•æ­·å²èª¿æ•´å›ç­”è©³ç´°ç¨‹åº¦
3. **ä¸Šä¸‹æ–‡ç†è§£**ï¼šç†è§£ä»£è©å’ŒæŒ‡ç¤ºè©åœ¨å°è©±ä¸­çš„å…·é«”æŒ‡å‘
4. **è¿½å•è™•ç†**ï¼šè­˜åˆ¥ä¸¦é©ç•¶å›æ‡‰è¿½å•æ€§è³ªçš„å•é¡Œ
5. **å°è©±è‡ªç„¶æ€§**ï¼šä½¿ç”¨è‡ªç„¶ã€å£èªåŒ–çš„è¡¨é”æ–¹å¼

ã€ç•¶å‰æª¢ç´¢è³‡æ–™ã€‘ï¼š
{context}

ã€ç•¶å‰ä½¿ç”¨è€…æå•ã€‘ï¼š
{query}

è«‹åŸºæ–¼ä»¥ä¸Šå°è©±æ­·å²å’Œæª¢ç´¢è³‡æ–™ï¼Œæä¾›ä¸€å€‹é€£è²«ã€è‡ªç„¶ã€å€‹äººåŒ–çš„å›æ‡‰ã€‚
"""
        
        try:
            response = await self.llm.ainvoke(prompt)
            logging.info(f"ç”Ÿæˆå›æ‡‰é•·åº¦: {len(response)}")
            return response
        except Exception as e:
            logging.error(f"å›æ‡‰ç”ŸæˆéŒ¯èª¤: {e}", exc_info=True)
            raise

    async def process_query_with_memory(self, query: str) -> str:
        """å¸¶æœ‰è¨˜æ†¶åŠŸèƒ½çš„æŸ¥è©¢è™•ç†"""
        try:
            logging.info(f"è™•ç†å¸¶è¨˜æ†¶çš„æŸ¥è©¢: {query}")
            
            results_with_scores = await self.retrieve_relevant_documents(query)
            filtered_results = [doc for doc, score in results_with_scores if score > self.RELEVANCE_SCORE_THRESHOLD]
            
            if results_with_scores:
                best_score = results_with_scores[0][1]
                logging.info(f"æª¢ç´¢åˆ° {len(results_with_scores)} å€‹æ–‡æª”ï¼Œæœ€ä½³åˆ†æ•¸: {best_score:.4f}")

            if not filtered_results:
                logging.warning(f"æŸ¥è©¢ '{query}' åœ¨ç›¸é—œæ€§éæ¿¾å¾Œç„¡çµæœ")
                suggestion = self._get_fallback_suggestion(query)
                
                # æª¢æŸ¥æ˜¯å¦ç‚ºè¿½å•
                if len(self.memory.conversation_history) > 0:
                    if any(word in query.lower() for word in ['æ›´å¤š', 'å…¶ä»–', 'é‚„æœ‰', 'è©³ç´°', 'é‚£', 'é€™']):
                        fallback_response = (f"æ ¹æ“šæˆ‘å€‘å‰›æ‰çš„å°è©±ï¼Œæˆ‘äº†è§£æ‚¨æƒ³çŸ¥é“æ›´å¤šè³‡è¨Šã€‚\n\n"
                                          f"é›–ç„¶æˆ‘åœ¨è³‡æ–™åº«ä¸­æ‰¾ä¸åˆ°æ›´è©³ç´°çš„ç›¸é—œè³‡æ–™ï¼Œä½†å»ºè­°æ‚¨ç›´æ¥è¯ç¹« **{suggestion['name']}** "
                                          f"ä»¥ç²å–æœ€å®Œæ•´çš„è³‡è¨Šï¼š\nğŸ”— [{suggestion['name']}]({suggestion['url']})")
                    else:
                        fallback_response = (f"âš  æŠ±æ­‰ï¼Œæˆ‘åœ¨è³‡æ–™åº«ä¸­æ‰¾ä¸åˆ°èˆ‡æ­¤å•é¡Œé«˜åº¦ç›¸é—œçš„ç­”æ¡ˆã€‚\n\n"
                                          f"å»ºè­°æ‚¨åƒè€ƒ **{suggestion['name']}** ç²å–æº–ç¢ºè³‡è¨Šï¼š\n"
                                          f"ğŸ”— [{suggestion['name']}]({suggestion['url']})")
                else:
                    fallback_response = (f"âš  ç³»çµ±ä¸­æŸ¥ç„¡æ­¤å•é¡Œçš„æ˜ç¢ºç­”æ¡ˆã€‚\n\n"
                                      f"å»ºè­°æ‚¨åƒè€ƒ **{suggestion['name']}** ç²å–æº–ç¢ºè³‡è¨Šï¼š\n"
                                      f"ğŸ”— [{suggestion['name']}]({suggestion['url']})")
                
                # è¨˜éŒ„åˆ°å°è©±æ­·å²
                self.memory.add_exchange(query, fallback_response, "fallback")
                return fallback_response
            
            context = "\n\n".join([
                f"ã€ä¾†æºã€‘{doc.metadata.get('source_url', 'ç„¡ç¶²å€')}\n{doc.page_content}"
                for doc in filtered_results
            ])
            
            logging.info(f"ä½¿ç”¨ {len(filtered_results)} å€‹æ–‡æª”ä½œç‚ºä¸Šä¸‹æ–‡")
            
            # ä½¿ç”¨å¸¶è¨˜æ†¶çš„å›æ‡‰ç”Ÿæˆ
            response = await self.generate_response_with_memory(query, context)
            
            # è¨˜éŒ„åˆ°å°è©±æ­·å²
            self.memory.add_exchange(query, response, f"æª¢ç´¢åˆ° {len(filtered_results)} å€‹æ–‡æª”")
            
            return response

        except Exception as e:
            error_msg = f"è™•ç†æ‚¨çš„å•é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"
            logging.error(f"æŸ¥è©¢è™•ç†éŒ¯èª¤: {e}", exc_info=True)
            return error_msg


async def main():
    st.set_page_config(
        page_title="æ±å³å¤§å­¸å°è©±å¼æ™ºèƒ½å•ç­”ç³»çµ±",
        page_icon="ğŸ“",
        layout="wide"
    )
    
    st.title("æ±å³å¤§å­¸å°è©±å¼æ™ºèƒ½å•ç­”ç³»çµ± ğŸ“")
    st.caption("ğŸ’¬ å…·å‚™å°è©±è¨˜æ†¶åŠŸèƒ½çš„æ™ºèƒ½åŠ©ç†")

    # ç³»çµ±ç‹€æ…‹æª¢æŸ¥
    with st.expander("ğŸ” ç³»çµ±ç‹€æ…‹æª¢æŸ¥", expanded=False):
        col1, col2, col3 = st.columns(3)
        
        with col1:
            model_exists = os.path.exists("./text2vec-large-chinese")
            if model_exists:
                required_files = ['config.json']
                model_files = os.listdir("./text2vec-large-chinese") if model_exists else []
                model_complete = any(f in model_files for f in ['pytorch_model.bin', 'model.safetensors'])
                st.success("âœ… åµŒå…¥æ¨¡å‹" if model_complete else "âš ï¸ æ¨¡å‹ä¸å®Œæ•´")
            else:
                st.error("âŒ åµŒå…¥æ¨¡å‹ç¼ºå¤±")
        
        with col2:
            db_exists = os.path.exists("chroma_db")
            st.success("âœ… å‘é‡è³‡æ–™åº«") if db_exists else st.error("âŒ å‘é‡è³‡æ–™åº«ç¼ºå¤±")
        
        with col3:
            device_info = "ğŸš€ GPU (MPS)" if torch.backends.mps.is_available() else "ğŸš€ GPU (CUDA)" if torch.cuda.is_available() else "ğŸ’» CPU"
            st.info(f"é‹ç®—è¨­å‚™: {device_info}")

    try:
        qa_system = SoochowQASystem()
        
        # å´é‚Šæ¬„åŠŸèƒ½
        with st.sidebar:
            st.header("ğŸ’¬ å°è©±ç®¡ç†")
            
            # é¡¯ç¤ºå°è©±æ­·å²çµ±è¨ˆ
            history_count = len(qa_system.memory.conversation_history)
            st.metric("å°è©±è¼ªæ•¸", history_count)
            
            # æ¸…é™¤å°è©±æ­·å²æŒ‰éˆ•
            if st.button("ğŸ—‘ï¸ æ¸…é™¤å°è©±æ­·å²"):
                qa_system.memory.clear_history()
                st.success("å°è©±æ­·å²å·²æ¸…é™¤ï¼")
                st.rerun()
            
            # é¡¯ç¤ºæœ€è¿‘çš„å°è©±
            if history_count > 0:
                st.subheader("ğŸ“‹ æœ€è¿‘å°è©±")
                for i, exchange in enumerate(qa_system.memory.conversation_history[-3:], 1):
                    with st.expander(f"ç¬¬{i}è¼ª: {exchange['user_input'][:20]}..."):
                        st.write(f"**å•é¡Œ:** {exchange['user_input']}")
                        st.write(f"**å›ç­”:** {exchange['system_response'][:200]}...")
                        st.caption(f"â° {exchange['timestamp']}")

        # ä¸»è¦è¼¸å…¥å€åŸŸ
        query = st.text_input(
            "ğŸ’­ è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ",
            placeholder="ä¾‹å¦‚ï¼šæ±å³å¤§å­¸è³‡æ–™ç§‘å­¸ç³»åœ¨å“ªï¼Ÿ",
            help="æ‚¨å¯ä»¥é€²è¡Œå¤šè¼ªå°è©±ï¼Œç³»çµ±æœƒè¨˜ä½æˆ‘å€‘çš„è«‡è©±å…§å®¹"
        )

        if query:
            with st.spinner('ğŸ¤” æ­£åœ¨æ€è€ƒä¸¦ç”Ÿæˆå›ç­”...'):
                response = await qa_system.process_query_with_memory(query)
                
                # é¡¯ç¤ºå›ç­”
                st.markdown("### ğŸ“ å›ç­”ï¼š")
                st.markdown(response, unsafe_allow_html=True)
                
                # å¦‚æœæœ‰å°è©±æ­·å²ï¼Œé¡¯ç¤ºç›¸é—œçš„æ­·å²å°è©±
                related_qa = qa_system.memory.get_related_previous_qa(query)
                if related_qa:
                    with st.expander("ğŸ“š ç›¸é—œçš„æ­·å²å°è©±"):
                        for i, qa in enumerate(related_qa, 1):
                            st.write(f"**ç›¸é—œå•é¡Œ{i}:** {qa['user_input']}")
                            st.write(f"**ç•¶æ™‚å›ç­”:** {qa['system_response'][:150]}...")
                            st.markdown("---")

    except Exception as e:
        st.error("âŒ ç³»çµ±ç™¼ç”Ÿæœªé æœŸçš„éŒ¯èª¤")
        with st.expander("éŒ¯èª¤è©³æƒ…"):
            st.code(str(e))
        logging.error(f"ä¸»ç¨‹å¼æœªé æœŸéŒ¯èª¤: {e}", exc_info=True)

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç³»çµ±å·²é—œé–‰")
        pass